"""
This module contains the HeyGPT Inference system.
"""

from __future__ import annotations

import os
import logging
import time
import transformers as ts
import torch

logger = logging.getLogger("uvicorn")

MODEL = os.getenv("HEY_MODEL", "PygmalionAI/pygmalion-2-13b")

try:
    Tokenizer = ts.AutoTokenizer.from_pretrained(MODEL)
    Pipeline = ts.pipeline(
        "text-generation",
        model=MODEL,
        tokenizer=Tokenizer,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
except Exception as e:
    logger.error("Could not load language model.")


def inference(input: str) -> str:
    """
    The inference function accepts an arbitrary string and returns a string
    according to the tokens generated by the language model.
    """
    start = time.time()
    seqs = Pipeline(input,
        max_new_tokens=256,
        do_sample=True,
        top_k=50,
        top_p=0.75,
        temperature=1.1,
        return_full_text=False,
        num_return_sequences=1,
        eos_token_id=Tokenizer.encode("\n"),
        pad_token_id=Tokenizer.pad_token_id,)
    text = seqs[0]["generated_text"]
    end = time.time()
    logger.info(f"Inference time: {end - start:.2f}s")
    return text

